---
title: 'Lecture 9: Unsupervised Learning Analysis of Cancer Cells'
author: "Andrew Valadez"
date: "May 2, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background
The goal of this hands-on session is for you to explore a complete analysis using the unsupervised learning techniques covered in the last class. You’ll extend what you’ve learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses. This expands on our RNA-Seq analysis from last day.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass. For example radius (i.e. mean of distances from center to points on the perimeter), texture (i.e. standard deviation of gray-scale values), and smoothness (local variation in radius lengths). Summary information is also provided for each group of cells including diagnosis (i.e. benign (not cancerous) and and malignant (cancerous)).
# Section 1
Preparing the data
```{r}
url <- "https://bioboot.github.io/bggn213_S18/class-material/WisconsinCancer.csv"

wisc.df <- read.csv(url, row.names = 1)
head(wisc.df)
```
How many diagsnosis are vcanver vs non cancer

```{r}
table(wisc.df$diagnosis)
```
Lets make a new data matrix with jsut the numeric values of nterest, ignore first 2 columns

```{r}
wisc.data <- as.matrix(wisc.df[,-c(1)])
```
Finally, setup a separate new vector called diagnosis to be 1 if a diagnosis is malignant ("M") and 0 otherwise. Note that R coerces TRUE to 1 and FALSE to 0.

```{r}
diagnosis <- as.numeric(wisc.df$diagnosis=="M")
```

Exploratory data analysis

1. There are 569 observation in this dataset
```{r}
dim(wisc.df)
```
How many variables in the data are suffixed with _mean?
2. 10
```{r}
grep("_mean", colnames(wisc.data), fixed = FALSE, value = TRUE)
length(grep("_mean", colnames(wisc.data), fixed = FALSE, value = TRUE))
```



How many of the observations are malignanat diagnosis? 
3. 212
```{r}
sum(diagnosis)
```

# Section 2
Performing PCA

```{r}
plot(colMeans(wisc.data))
plot(apply(wisc.data, 2, sd))
```
```{r}
dim(wisc.data)
```


```{r}
#had to do scale = true because on the plots above the 4 and 24 posiiton needed to be scaled to the rest of them

wisc.pr <- prcomp(wisc.data, scale. = TRUE)
summary(wisc.pr)
```

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
44%
Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
3
Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
7



Interpreing PCA results

```{r}
biplot(wisc.pr)
```


```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis+1, xlab="PC1", ylab="PC2")
#need diagnosis +1 because 0 defaults to a white circle, everything in red is cancer, 

```
can draw a line down it and see not much variability in the black, there are many diff types of cancers in red which makes sense because diff types of cancers 


```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis+1, xlab="PC1", ylab="PC3")
```



get the elbow of data called a scree plot
```{r}
# Variance explained by each principal component: pve
pve <- wisc.pr$sdev^2/ 30

# Plot variance explained for each principal component
bell <- plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
# Plot cumulative proportion of variance explained
taco <- plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

#Section 3
Hierarchial clustering of case data


```{r}
data.scaled <- scale(wisc.data)
```


```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
abline(h=20, col="red")

```
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```
how do these groups match our diagndosin
```{r}
table( diagnosis)
```

```{r}
table(wisc.hclust.clusters)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?
I think 8 gives a new interesting view of it
```{r}
wisc.hclust.clusters210 <- cutree(wisc.hclust,k=8)
table(wisc.hclust.clusters210, diagnosis)
```

# Section 4

```{r}
data.scaled <- scale(wisc.data)
wisc.km <- kmeans(data.scaled, centers= 2, nstart= 20)
table(wisc.km$cluster)
table(wisc.km$cluster, diagnosis)
```

Q13. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

pretty similar to the k means 

#Section 5

CLustering on PCA results

here we clustering on the PCS , got a clean tree and clean cluster
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:3]), method = "ward.D2")
plot(wisc.pr.hclust.clusters)
```
basically just cluster in the space of the top x amount of pca, pca is often used as first step for more machine learning, used as a filtering step
```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters, diagnosis)

#lets color our results
plot(wisc.pr$x[,1:2], col=wisc.pr.hclust.clusters)

```

```{r}
install.packages("rgl")
```

```{r}
library(rgl)
```

```{r}
plot3d(wisc.pr$x[,1:2], col=wisc.pr.hclust.clusters)
```



# this is cool will predict where its at
```{r}
url2 <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url2)
npc <- predict(wisc.pr, newdata=new)

plot(wisc.pr$x[,1:2], col=wisc.pr.hclust.clusters)
points(npc[,1], npc[,2], col=c("blue", "purple"), pch=16, cex=3)
```

